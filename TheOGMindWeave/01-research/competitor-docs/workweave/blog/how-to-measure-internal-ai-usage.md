# How to Measure Internal AI Usage - Remote

> Weave combines LLMs and domain-specific machine learning to understand engineering work. We understand how much work was done by AI vs. humans. How much AI is helping your team ship faster, if it's having an impact on code quality and code reviews.

## 1. ## ### ### ### ## - - ## - ### - ### - ### - ### ## - ## ## $4.2M seed round led by Moonfire, Burst Capital & Y Combinator

$4.2M seed round led by Moonfire, Burst Capital & Y Combinator

Here's something that's been keeping me up at night lately... and I bet it's doing the same to you.

Your engineering teams are using AI tools left and right. ChatGPT for debugging. Copilot for code generation. Claude for documentation. Maybe even custom LLMs for specific tasks.

But here's the million-dollar question:**How do you actually measure what's working?**

Most engineering leaders I talk to are flying blind. They know their teams are using AI, but they can't tell you which tools are moving the needle on productivity, which ones are creating technical debt, or whether that expensive enterprise AI subscription is worth renewing.

Sound familiar?

Why Measuring AI Usage Isn't Just Nice-to-Have Anymore

Let's be real for a second. AI isn't some experimental side project anymore. According toStack Overflow's 2024 Developer Survey[1], 76% of developers are already using AI tools in their workflow. That's not a trend – that's the new normal.

But here's where it gets tricky...

Unlike traditional dev tools where you can measure impact through clear metrics (build times, deployment frequency, etc.), AI usage creates this weird gray area. How do you quantify "better code quality from AI assistance" or "faster problem-solving with AI pair programming"?

The Current Approach (And Why It's Not Working)

Most teams are trying to measure AI usage through surface-level metrics:

Number of AI tool licenses purchased
- How often developers log into AI platforms
- Basic usage statistics from tool dashboards

**The problem?**These metrics tell you nothing about actual impact.

It's like measuring a car's performance by counting how many times you turn the key. Sure, it shows activity, but does it tell you if you're getting where you need to go faster?

A Better Framework for Measuring AI Usage

Here's what actually works (and I've seen this approach transform teams):

1.**Start with Output-Based Metrics**

Don't measure AI usage directly. Measure what AI usage should improve:

**Code velocity**: Lines of meaningful code shipped per sprint
- **Bug reduction**: Defect rates in AI-assisted vs. non-assisted code
- **Review efficiency**: Time from PR creation to merge
- **Documentation quality**: Completeness and clarity scores

2.**Track Context-Aware Usage Patterns**

This is where tools like**Weave**become invaluable. Instead of just knowing "Sarah used Copilot 50 times this week," you need to understand:

Which types of tasks benefit most from AI assistance
- Where AI tools are creating bottlenecks or confusion
- How AI usage correlates with individual developer strengths and weaknesses
- Time investment patterns around AI-assisted work

3.**Implement Qualitative Feedback Loops**

Numbers tell part of the story. Developer experience tells the rest:

**Weekly AI retrospectives**: What worked? What didn't?
- **Code review comments**: Are AI-generated solutions creating more discussion?
- **Pair programming observations**: How does AI change collaboration dynamics?

4.**Monitor Technical Debt Impact**

This one's crucial and often overlooked. AI tools can generate code fast, but is it*good*code?

**Code complexity metrics**: Are AI-assisted files harder to maintain?
- **Test coverage**: Is AI helping or hurting testing practices?
- **Refactoring frequency**: How often do teams need to clean up AI-generated code?

The Tools That Actually Help

You'll need a combination of approaches:

**For comprehensive engineering analytics**, platforms like**Weave**excel at connecting AI usage patterns to actual team performance. Their LLM-powered analysis can identify which AI tools are genuinely improving your team's output versus which ones are just creating busy work.

**For direct AI tool monitoring**:

GitHub Copilot Analytics (if you're using Copilot)
- Custom tracking through API calls for tools like OpenAI
- Browser extension monitoring for web-based AI tools

**For code quality tracking**:

SonarQube for technical debt metrics
- CodeClimate for maintainability scores
- Custom scripts to analyze AI-generated code patterns

Common Pitfalls to Avoid

Don't Fall Into the SPACE Trap

SPACE metrics (Satisfaction, Performance, Activity, Communication, Efficiency) sound great in theory, but they're expensive to calculate and often don't provide actionable insights for AI usage specifically. Many teams get bogged down in complex measurement frameworks when simpler approaches would serve them better.

Avoid Vanity Metrics

"AI tool usage up 200%" means nothing if code quality is declining or developers are getting frustrated. Focus on outcomes, not activity.

Don't Ignore the Human Factor

AI tools are only as good as the people using them. Measure training effectiveness and adoption barriers, not just raw usage numbers.

Making It Actionable

Here's your practical next steps:

**Start small**: Pick 2-3 key metrics that matter most to your team's goals
1. **Establish baselines**: Measure current performance before optimizing AI usage
1. **Create feedback loops**: Weekly check-ins to discuss what's working
1. **Iterate quickly**: Adjust your measurement approach based on what you learn

The goal isn't perfect measurement – it's**useful measurement**that helps you make better decisions about AI adoption and optimization.

The Bottom Line

Measuring AI usage effectively isn't about tracking every click and keystroke. It's about understanding whether AI tools are genuinely making your team more effective at solving real problems.

**Weave's**approach of using domain-specific machine learning to analyze engineering work patterns provides the kind of deep insights that can actually guide AI strategy decisions. Rather than just showing you what happened, it helps you understand why certain AI usage patterns lead to better outcomes.

Ready to move beyond surface-level AI metrics and start measuring what actually matters? The teams that figure this out first will have a massive competitive advantage in the AI-powered development landscape.

What's the one AI usage metric you wish you could measure but haven't figured out how to track yet?

Article written by

Brennan Lupyrypa

Make AI Engineering Simple

Effortless charts, clear scope, easy code review, and team analysis

Book a demo

Book a demo

Book a demo

Book a demo

Book a demo

Book a demo