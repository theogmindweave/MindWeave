# Most Frequently Asked Questions By Engineering Managers (about Weave) - Remote

> Weave combines LLMs and domain-specific machine learning to understand engineering work. We understand how much work was done by AI vs. humans. How much AI is helping your team ship faster, if it's having an impact on code quality and code reviews.

### ### - ### ### - ### ## ### ### ### ### ## - ### ### - ### ### ## - ### - ### - ### ### ## - ### ### - ### - ### ## ### ### ### - ### - ### ## ### ### - ### ### - ### ## ### ### ### ### ### - ### ## ### ### - ### ### $4.2M seed round led by Moonfire, Burst Capital & Y Combinator

$4.2M seed round led by Moonfire, Burst Capital & Y Combinator

What is Weave?

Weave is an engineering analytics platform that helps teams measure AI adoption and optimize engineering productivity. We connect your development tools (GitHub, Cursor, Claude, and other AI tools) to provide team benchmarks, track AI ROI, and help you understand how AI is impacting your engineering velocity.

Who is Weave built for?

Weave is designed for CTOs, VPs of Engineering, and engineering leaders who need to:

Measure AI tool adoption across their organization
- Calculate ROI on AI development tool investments
- Benchmark their team's productivity against industry standards
- Make data-driven decisions about engineering tool spend
- Optimize team velocity and productivity

What problem does Weave solve?

As companies invest thousands or millions in AI development tools like Cursor, GitHub Copilot, and Claude, engineering leaders need answers to critical questions: Are developers actually using these tools? Is AI improving productivity? Which teams are getting the most value? Where should we invest more? Weave provides the data and insights to answer these questions with confidence.

How is Weave different from traditional engineering analytics platforms?

Weave is built specifically for the AI-native engineering era. While traditional platforms focus on DORA metrics and sprint velocity, Weave adds a critical layer: AI adoption analytics, AI-generated code quality measurement, and AI tool ROI tracking. We integrate directly with AI coding tools to provide insights no other platform can offer.

**Features & Metrics**

What metrics does Weave track?

Weave provides comprehensive visibility into:

**AI Adoption**: Usage rates across your team, by individual, team, and organization
- **AI ROI**: Time saved, productivity gains, and cost justification
- **Engineering Velocity**: Throughput, cycle time, and delivery metrics
- **Code Quality**: AI-generated code quality and review outcomes
- **Team Benchmarks**: Compare your performance against similar organizations
- **Tool Effectiveness**: Which AI tools deliver the most value for your team

How does Weave calculate AI ROI?

Weave measures the time saved, code quality improvements, and velocity gains from AI tools, then compares this against your investment in AI subscriptions and training. We provide concrete data showing which tools deliver positive ROI and which may need optimization or reconsideration.

What are team benchmarks and why do they matter?

Our benchmarks show how your engineering team compares to similar organizations across AI adoption rates, productivity metrics, and engineering velocity. This helps you understand if you're ahead or behind the curve, identify improvement opportunities, and set realistic targets based on what high-performing teams achieve.

Can Weave track multiple AI tools simultaneously?

Yes. Weave tracks usage across your entire AI tool stack simultaneously, giving you a complete view of your AI investments and helping you understand which tools provide the most value for different use cases and team members.

What is the Deep Research Agent?

The Deep Research Agent uses advanced AI to perform in-depth analysis of your engineering data. It can answer complex questions about your team's productivity, identify patterns, and provide actionable recommendations tailored to your organization.

Does Weave provide AI code review insights?

Yes. Weave analyzes how AI is being used in code reviews, the quality of AI-generated code, and how AI impacts your review process, cycle times, and overall code quality.

**Integrations & Implementation**

What tools does Weave integrate with?

Weave currently integrates with:

**GitHub**: Repository activity, commits, PRs, code reviews
- **Cursor**: AI coding assistant usage and effectiveness
- **Claude**: AI assistant interactions
- Additional AI development tools (continuously expanding)

How long does implementation take?

Most teams are up and running within a day. Integration setup takes minutes, and Weave immediately begins collecting data. You'll see meaningful insights within 24-48 hours.

What access does Weave require?

Weave requires read-only access to your integrated tools to collect analytics data. We follow security best practices and request only the minimum permissions necessary. Specific requirements:

GitHub: Read access to repositories and activity metadata
- AI Tools: Usage analytics and interaction data

We do NOT access your actual source code or proprietary information.

Does Weave work with on-premise or self-hosted systems?

Contact our team to discuss on-premise or private cloud deployment options for enterprise requirements.

Can Weave integrate with our custom tools?

We're open to discussing custom integrations based on your needs. Reach out to our technical team to explore possibilities.

**Data Security & Privacy**

What data does Weave collect?

Weave collects analytics metadata including:

Development activity (commits, PRs, reviews - metadata only)
- AI tool usage patterns and frequency
- Velocity and productivity metrics
- Collaboration data

How secure is Weave?

Weave follows industry-standard security practices:

Encryption in transit and at rest
- Secure authentication protocols
- Minimal permission requirements
- Regular security audits
- SOC 2 Type II compliance (contact us for detailed compliance documentation)

Who can access my team's data?

Only authorized users within your organization can access your Weave data. Benchmark data is anonymized and aggregated—no other company can see your specific metrics.

Where is data stored?

Contact our team for specific information about data residency, storage locations, and compliance with regional data requirements.

Can we export our data?

Yes. Contact our customer success team for information about data export capabilities and formats.

**Use Cases**

How do CTOs use Weave to justify AI tool budgets?

Weave provides concrete data on:

Time saved through AI tools (hours per developer per week)
- Productivity improvements (percentage gains in velocity)
- Code quality impact (review outcomes, bug rates)
- Comparative ROI across different AI tools
- Adoption rates and engagement metrics

This data helps you justify current budgets to finance and leadership, and make informed decisions about renewals, expansions, or optimization.

How can Weave improve AI adoption across our team?

Weave shows you exactly who is using AI tools, how often, and how effectively. This visibility helps you:

Identify teams or individuals who need training or support
- Find champions who can mentor others
- Understand barriers to adoption
- Measure the impact of training initiatives
- Target resources where they'll have the most impact

Can Weave help identify which AI tools to invest in?

Yes. By tracking usage, effectiveness, and ROI across your entire AI tool stack, Weave shows you which tools deliver value and which underperform. This helps you optimize your tool portfolio and avoid waste.

How do engineering leaders use Weave for performance optimization?

Leaders use Weave to:

Identify productivity bottlenecks
- Understand which workflows benefit most from AI
- Benchmark teams against each other and industry standards
- Make data-driven decisions about process improvements
- Track the impact of productivity initiatives over time

**AI Measurement Specifics**

How does Weave determine if code is AI-generated?

Weave uses sophisticated analysis combining multiple signals from your integrated AI tools, code metadata, and usage patterns to accurately identify and measure AI-generated code contributions.

What's considered "good" AI adoption?

Based on our benchmark data:

**High performers**: 80%+ of developers actively using AI tools
- **AI-generated code**: 20-40% of total codebase
- **Acceptance rates**: 30%+ for AI suggestions
- **Consistency**: Regular daily usage, not sporadic

Your specific targets depend on your team, domain, and maturity level.

How do you measure AI-generated code quality?

We analyze AI-generated code through multiple dimensions:

Code review outcomes and approval rates
- Revision frequency and refactoring needs
- Bug rates and production issues
- Integration quality with existing codebase
- Comparison against human-written code

What's the typical ROI timeline for AI tools?

Based on Weave customer data, teams typically see:

**15-35%**improvement in coding speed
- **20-40%**reduction in time on routine tasks
- **10-25%**increase in overall productivity
- **Positive ROI**within 3-6 months

Actual results depend on team size, AI tool selection, adoption rates, and implementation approach.

**Pricing & Enterprise**

How is Weave priced?

Weave offers multiple pricing tiers based on team size and feature requirements.

Do you offer enterprise plans?

Yes. Enterprise plans include:

Unlimited seats and data retention
- Advanced features including Deep Research Agent
- Custom reporting and analytics
- Dedicated customer success manager
- Priority support
- Custom integrations (if needed)

Is there a free trial or pilot program?

Contact our team to discuss trial options and proof-of-concept programs for evaluating Weave with your team.

What's included in onboarding?

Our onboarding includes:

Integration setup assistance
- Baseline metric establishment
- Dashboard configuration
- Team training and best practices
- Initial data analysis and recommendations
- Regular check-ins during the first month

**Technical Details**

Does Weave offer an API?

Yes - contact our technical team for API documentation and access information for custom integrations or data exports.

Can Weave scale to large engineering organizations?

Yes. Weave scales to enterprises with hundreds of developers across multiple teams, locations, and time zones.

How does Weave handle data for remote and distributed teams?

Weave tracks digital activity through GitHub and AI tools, so location is irrelevant. The platform works seamlessly for remote, hybrid, and in-office teams.

Can we track multiple GitHub organizations or workspaces?

Yes. Contact our team to discuss multi-organization setups and consolidated reporting.

**Support & Getting Started**

What kind of support do you provide?

Weave provides comprehensive support including:

Technical implementation assistance
- Data analysis and interpretation
- Best practices guidance
- Regular business reviews (enterprise plans)
- Documentation and training resources

How quickly can we see results?

Data collection begins immediately upon integration. You'll see initial insights within 24-48 hours and meaningful trends within 1-2 weeks as baseline data accumulates.

What do we need to get started?

To implement Weave, you'll need:

Admin access to your GitHub organization
- Access to your team's AI development tools
- Basic team structure information
- Clear goals for what you want to measure

Can we see a demo?

Yes. Visit**workweave.ai**or contact our team directly to schedule a demo tailored to your organization's needs.

What if we need custom features or reports?

Weave actively incorporates customer feedback into our product roadmap. Contact our team to discuss custom requirements—we're committed to delivering the insights you need to make informed decisions.

Article written by

Brennan Lupyrypa

Make AI Engineering Simple

Effortless charts, clear scope, easy code review, and team analysis

Book a demo

Book a demo

Book a demo

Book a demo

Book a demo

Book a demo